{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "-- Logistic Regression:\n",
        "\n",
        "It is a classification algorithm used to predict categorical outcomes (often binary, like 0/1, Yes/No, True/False).\n",
        "\n",
        "Instead of predicting exact values, it predicts the probability that an observation belongs to a particular class.\n",
        "\n",
        "Uses the logistic (sigmoid) function to map predicted values to a range between 0 and 1:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        ")\n",
        "P(Y=1)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X)\n",
        "1\n",
        "\t​\n",
        "\n",
        "\n",
        "Linear Regression:\n",
        "\n",
        "It is a regression algorithm used to predict continuous numerical outcomes.\n",
        "\n",
        "Predicts the value of a dependent variable using a linear relationship:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X+ϵ\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Feature\tLinear Regression\tLogistic Regression\n",
        "Output\tContinuous values\tProbability (0–1) / class label\n",
        "Purpose\tRegression (predict value)\tClassification (predict class)\n",
        "Equation\tLinear\tSigmoid of linear equation\n",
        "Error Measure\tMSE (Mean Squared Error)\tLog-Loss / Cross-Entropy\n",
        "\n",
        "\n",
        "#Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "\n",
        "-- Role of the Sigmoid Function in Logistic Regression:\n",
        "\n",
        "Definition:\n",
        "The sigmoid (or logistic) function is:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "1\n",
        "\t​\n",
        "\n",
        "\n",
        "Where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "z=β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X.\n",
        "\n",
        "Maps Any Value to Probability:\n",
        "\n",
        "Linear combination\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "z=β\n",
        "0\n",
        "\t​\n",
        "\n",
        "+β\n",
        "1\n",
        "\t​\n",
        "\n",
        "X can be any real number\n",
        "(\n",
        "−\n",
        "∞\n",
        ",\n",
        "+\n",
        "∞\n",
        ")\n",
        "(−∞,+∞).\n",
        "\n",
        "Sigmoid transforms\n",
        "𝑧\n",
        "z into a value between 0 and 1, which can be interpreted as a probability of the positive class.\n",
        "\n",
        "Decision Boundary:\n",
        "\n",
        "Typically, if\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "σ(z)≥0.5, predict class 1; otherwise, predict class 0.\n",
        "\n",
        "This threshold allows Logistic Regression to classify data points.\n",
        "\n",
        "Non-linearity:\n",
        "\n",
        "The sigmoid function introduces non-linear mapping, enabling Logistic Regression to handle classification even if the input-output relationship is not strictly linear.\n",
        "\n",
        "#Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "-- Regularization in Logistic Regression:\n",
        "\n",
        "Definition:\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from fitting the training data too perfectly, which helps it generalize better to unseen data.\n",
        "\n",
        "Why It’s Needed:\n",
        "\n",
        "Logistic Regression can overfit when there are many features or when features are highly correlated.\n",
        "\n",
        "Overfitting leads to high variance, meaning the model performs well on training data but poorly on new data.\n",
        "\n",
        "Regularization helps simplify the model by shrinking coefficients.\n",
        "\n",
        "Types of Regularization:\n",
        "\n",
        "L1 Regularization (Lasso): Adds the sum of absolute values of coefficients as a penalty. Can produce sparse models (some coefficients become zero).\n",
        "\n",
        "Loss\n",
        "=\n",
        "−\n",
        "Log-Likelihood\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Loss=−Log-Likelihood+λ∑∣β\n",
        "j\n",
        "\t​\n",
        "\n",
        "∣\n",
        "\n",
        "L2 Regularization (Ridge): Adds the sum of squared coefficients as a penalty. Shrinks coefficients but usually keeps all of them.\n",
        "\n",
        "Loss\n",
        "=\n",
        "−\n",
        "Log-Likelihood\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Loss=−Log-Likelihood+λ∑β\n",
        "j\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "Effect:\n",
        "\n",
        "Reduces overfitting.\n",
        "\n",
        "Improves model generalization.\n",
        "\n",
        "Controls the magnitude of coefficients.\n",
        "\n",
        "#Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "-- Common Evaluation Metrics for Classification Models:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Definition: Percentage of correct predictions out of total predictions.\n",
        "\n",
        "Accuracy\n",
        "=\n",
        "TP + TN\n",
        "TP + TN + FP + FN\n",
        "Accuracy=\n",
        "TP + TN + FP + FN\n",
        "TP + TN\n",
        "\t​\n",
        "\n",
        "\n",
        "Use: Good for balanced datasets.\n",
        "\n",
        "Limitation: Misleading for imbalanced datasets.\n",
        "\n",
        "Precision\n",
        "\n",
        "Definition: Out of all predicted positives, how many are actually positive.\n",
        "\n",
        "Precision\n",
        "=\n",
        "TP\n",
        "TP + FP\n",
        "Precision=\n",
        "TP + FP\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Use: Important when false positives are costly (e.g., spam detection).\n",
        "\n",
        "Recall (Sensitivity)\n",
        "\n",
        "Definition: Out of all actual positives, how many were correctly predicted.\n",
        "\n",
        "Recall\n",
        "=\n",
        "TP\n",
        "TP + FN\n",
        "Recall=\n",
        "TP + FN\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Use: Important when false negatives are costly (e.g., disease detection).\n",
        "\n",
        "F1-Score\n",
        "\n",
        "Definition: Harmonic mean of Precision and Recall.\n",
        "\n",
        "F1\n",
        "=\n",
        "2\n",
        "⋅\n",
        "Precision\n",
        "⋅\n",
        "Recall\n",
        "Precision + Recall\n",
        "F1=2⋅\n",
        "Precision + Recall\n",
        "Precision⋅Recall\n",
        "\t​\n",
        "\n",
        "\n",
        "Use: Balances Precision and Recall, useful for imbalanced datasets.\n",
        "\n",
        "ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
        "\n",
        "Definition: Measures model’s ability to distinguish between classes at all thresholds.\n",
        "\n",
        "Use: Higher AUC = better separation between positive and negative classes.\n",
        "\n",
        "Why Metrics Are Important:\n",
        "\n",
        "They provide different perspectives on model performance.\n",
        "\n",
        "Help choose the right model for the problem.\n",
        "\n",
        "Essential for imbalanced datasets, where accuracy alone is misleading.\n",
        "\n"
      ],
      "metadata": {
        "id": "h7RSi9stDjP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.(Use Dataset from sklearn package).(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# For simplicity, make it a binary classification (class 0 vs class 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA4k210hE8px",
        "outputId": "42ebbe1b-458a-440d-ec68-7b06cf273fbf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.(Use Dataset from sklearn package).(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Make predictions and print accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4FuY199FY80",
        "outputId": "3834e210-7539-4a15-990b-4c273989d690"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "sepal length (cm): -0.4054\n",
            "sepal width (cm): 0.8689\n",
            "petal length (cm): -2.2779\n",
            "petal width (cm): -0.9568\n",
            "\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.(Use Dataset from sklearn package).(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDCyyiMkF0X_",
        "outputId": "5c686dc7-52f7-4e1b-a8aa-70664928556e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      0.85      0.92        13\n",
            "   virginica       0.87      1.00      0.93        13\n",
            "\n",
            "    accuracy                           0.96        45\n",
            "   macro avg       0.96      0.95      0.95        45\n",
            "weighted avg       0.96      0.96      0.96        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.(Use Dataset from sklearn package).(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=200)  # liblinear supports both L1 and L2\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i49S_jiyGI4W",
        "outputId": "c5bfff9e-4dcd-4404-d0d6-038d535a46fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Validation Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.(Use Dataset from sklearn package).(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --------- Logistic Regression WITHOUT scaling ---------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --------- Logistic Regression WITH standardization ---------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy WITHOUT scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy WITH scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjf0kI3cGfGv",
        "outputId": "ac71d8bd-946d-495f-fe3a-b5a79f1261a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 1.0\n",
            "Accuracy WITH scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "\n",
        "-- 1. Data Handling\n",
        "\n",
        "Collect and clean data: Ensure there are no missing values, duplicates, or erroneous entries.\n",
        "\n",
        "Feature selection/engineering: Create meaningful features (e.g., past purchase history, browsing behavior, demographics).\n",
        "\n",
        "Categorical encoding: Convert categorical variables into numerical format (e.g., One-Hot Encoding, Target Encoding).\n",
        "\n",
        "2. Feature Scaling\n",
        "\n",
        "Scale numerical features using StandardScaler or MinMaxScaler.\n",
        "\n",
        "Logistic Regression benefits from scaling because it improves gradient convergence and coefficient interpretability.\n",
        "\n",
        "3. Handle Class Imbalance\n",
        "\n",
        "Since only 5% of customers respond:\n",
        "\n",
        "Option 1: Resampling\n",
        "\n",
        "Oversample minority class: e.g., using SMOTE (Synthetic Minority Oversampling Technique).\n",
        "\n",
        "Undersample majority class: Randomly reduce non-responders.\n",
        "\n",
        "Option 2: Use class weights\n",
        "\n",
        "Logistic Regression allows class_weight='balanced', which penalizes misclassification of minority class more heavily.\n",
        "\n",
        "4. Model Building & Hyperparameter Tuning\n",
        "\n",
        "Train Logistic Regression with L1 or L2 regularization.\n",
        "\n",
        "Tune hyperparameters using GridSearchCV or RandomizedSearchCV:\n",
        "\n",
        "C (regularization strength)\n",
        "\n",
        "penalty (L1 or L2)\n",
        "\n",
        "solver compatible with your penalty\n",
        "\n",
        "Include stratified cross-validation to maintain class distribution in folds.\n",
        "\n",
        "5. Model Evaluation Metrics\n",
        "\n",
        "Accuracy is not reliable due to imbalance. Use:\n",
        "\n",
        "Precision: Of predicted responders, how many are actual responders?\n",
        "\n",
        "Recall (Sensitivity): Of all actual responders, how many did we identify?\n",
        "\n",
        "F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "ROC-AUC: Measures how well the model separates responders vs. non-responders.\n",
        "\n",
        "PR-AUC (Precision-Recall AUC): Especially useful for imbalanced datasets.\n",
        "\n",
        "6. Deployment Considerations\n",
        "\n",
        "Threshold tuning: Adjust the probability threshold for predicting a responder to optimize business goals (e.g., maximize campaign ROI).\n",
        "\n",
        "Monitor model drift: Customer behavior may change over time, so periodically retrain the model.\n",
        "\n",
        "Explainability: Use feature importance or coefficients to understand what drives responses, which helps marketing strategy.\n",
        "\n",
        "✅ Summary Approach:\n",
        "\n",
        "Clean and engineer features.\n",
        "\n",
        "Scale numerical features.\n",
        "\n",
        "Handle imbalance via SMOTE or class weights.\n",
        "\n",
        "Train Logistic Regression with cross-validation and hyperparameter tuning.\n",
        "\n",
        "Evaluate using precision, recall, F1-score, ROC-AUC, and PR-AUC.\n",
        "\n",
        "Deploy with threshold tuning and monitor performance."
      ],
      "metadata": {
        "id": "NLG48lUSH2YB"
      }
    }
  ]
}